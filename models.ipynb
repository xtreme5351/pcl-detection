{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d37472",
   "metadata": {},
   "source": [
    "## Novel model approach\n",
    "\n",
    "This notebook serves as the source code for all the model testing and training (along with hyperparam grid search) before the development/submission of the final best model. This model approach tries a variation on the transformer architecture, with different heads, as detailed in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e213e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043011bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcl_tf.dataset_manager import DatasetManager as DM\n",
    "from pcl_tf.collation import collate_fn\n",
    "from pcl_tf.tf import warmup_model, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9fd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 7\n",
    "LOAD_BATCH_SIZE = 64\n",
    "LOCAL_CACHE_DIR = './models_cache'\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = False\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd961ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up tokenizer...\n",
      "Warming up encoder (downloads model if needed)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20152bac7add49539e1609840f85344d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mAlbertModel LOAD REPORT\u001b[0m from: albert-base-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "predictions.LayerNorm.bias   | UNEXPECTED |  | \n",
      "predictions.bias             | UNEXPECTED |  | \n",
      "predictions.dense.bias       | UNEXPECTED |  | \n",
      "predictions.LayerNorm.weight | UNEXPECTED |  | \n",
      "predictions.dense.weight     | UNEXPECTED |  | \n",
      "predictions.decoder.bias     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cache warmup completed.\n"
     ]
    }
   ],
   "source": [
    "print('Warming up tokenizer...')\n",
    "tokenizer = get_tokenizer(MODEL_NAME)\n",
    "\n",
    "print('Warming up encoder (downloads model if needed)...')\n",
    "\n",
    "_ = warmup_model(MODEL_NAME, device=None, cache_dir=LOCAL_CACHE_DIR)\n",
    "print('Model cache warmup completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f976d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_path = \"data/train_semeval_parids-labels.csv\"\n",
    "dev_labels_path = \"data/dev_semeval_parids-labels.csv\"\n",
    "texts_path = \"data/dontpatronizeme_pcl_cleaned.csv\"\n",
    "test_path = \"data/task4_test.tsv\"\n",
    "cats_path = \"data/dontpatronizeme_categories.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07510058",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv(texts_path, low_memory=False)\n",
    "texts_df[\"par_id\"] = texts_df[\"par_id\"].astype(int)\n",
    "texts_df = texts_df.set_index(\"par_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89398ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8375\n",
      "Binary distribution: [7581  794]\n",
      "Multilabel distribution: [574. 160. 162. 192. 145. 363.  29.]\n"
     ]
    }
   ],
   "source": [
    "training_ds = DM(train_labels_path, texts_df=texts_df)\n",
    "training_ds.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aee088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 label par_id(s) missing from texts_df. Example missing ids: [8640]\n",
      "Total samples: 2093\n",
      "Binary distribution: [1894  199]\n",
      "Multilabel distribution: [142.  36.  62.  38.  52. 106.  11.]\n"
     ]
    }
   ],
   "source": [
    "dev_ds = DM(dev_labels_path, texts_df=texts_df)\n",
    "dev_ds.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff0054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(MODEL_NAME)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training_ds, batch_size=LOAD_BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=lambda b: collate_fn(tokenizer, b), pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "\n",
    "dev_loader = DataLoader(dev_ds, batch_size=LOAD_BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=lambda b: collate_fn(tokenizer, b), pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66fc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dev(model, dataloader, device):\n",
    "    model.eval()\n",
    "    bin_probs=[]\n",
    "    bin_labels=[]\n",
    "    multi_probs=[]\n",
    "    multi_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in dataloader:\n",
    "            input_ids = b[\"input_ids\"].to(device)\n",
    "            attention_mask = b[\"attention_mask\"].to(device)\n",
    "            labels = b[\"labels\"].to(device)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bin_probs.append(torch.sigmoid(out[\"logit_bin\"]).cpu().numpy())\n",
    "            multi_probs.append(torch.sigmoid(out[\"logit_multi\"]).cpu().numpy())\n",
    "            bin_labels.append(labels[:,0].cpu().numpy())\n",
    "            multi_labels.append(labels[:,1:].cpu().numpy())\n",
    "            \n",
    "    bin_probs = np.concatenate(bin_probs); bin_labels = np.concatenate(bin_labels)\n",
    "    multi_probs = np.concatenate(multi_probs); multi_labels = np.concatenate(multi_labels)\n",
    "    micro_f1 = f1_score(multi_labels.flatten(), (multi_probs>=0.5).astype(int).flatten(), zero_division=0)\n",
    "    return {\"micro_f1\": micro_f1, \"bin_ap\": average_precision_score(bin_labels, bin_probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9616c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcl_tf.tf import PCLModel\n",
    "\n",
    "def train_and_eval(config):\n",
    "    model = PCLModel(config[\"model_name\"], n_labels=NUM_LABELS, dropout=config[\"dropout\"], device=DEVICE).to(DEVICE)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
    "    print(\"Model and optimizer created\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "            optim.zero_grad()\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running += loss.item()\n",
    "        # optionally print per-epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running/len(train_loader)}\")\n",
    "    metrics = evaluate_dev(model, dev_loader, DEVICE)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"model_name\": [\"albert-base-v2\", \"roberta-base\"],\n",
    "    \"lr\": [2e-5, 5e-5, 1e-4, 5e-4],\n",
    "    \"wd\": [1e-3, 1e-2, 5e-3],\n",
    "    \"max_len\": [128, 256],\n",
    "    \"dropout\": [0, 0.1, 0.01],\n",
    "    \"epochs\": [6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1898784",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   1%|▏         | 1/72 [00:00<00:57,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 57.50 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.87 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 15.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   3%|▎         | 2/72 [00:01<00:43,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 57.50 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.87 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 15.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   4%|▍         | 3/72 [00:01<00:39,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 64.31 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.87 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 15.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   6%|▌         | 4/72 [00:02<00:36,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 66.25 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.87 GiB memory in use. Of the allocated memory 6.69 GiB is allocated by PyTorch, and 15.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   7%|▋         | 5/72 [00:02<00:34,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 65.25 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   8%|▊         | 6/72 [00:03<00:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 64.44 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  10%|▉         | 7/72 [00:03<00:32,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 63.88 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  11%|█         | 8/72 [00:04<00:31,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 64.44 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _afterFork at 0x745322dbb920>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.13/logging/__init__.py\", line 245, in _afterFork\n",
      "    def _afterFork():\n",
      "KeyboardInterrupt: \n",
      "Grid:  12%|█▎        | 9/72 [00:04<00:30,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 64.94 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  14%|█▍        | 10/72 [00:05<00:29,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 65.25 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.91 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 15.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  15%|█▌        | 11/72 [00:05<00:30,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 75.00 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.96 GiB memory in use. Of the allocated memory 6.79 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  17%|█▋        | 12/72 [00:06<00:29,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 74.25 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.96 GiB memory in use. Of the allocated memory 6.79 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  18%|█▊        | 13/72 [00:06<00:29,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 74.62 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.96 GiB memory in use. Of the allocated memory 6.79 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  19%|█▉        | 14/72 [00:07<00:28,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 79.31 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.96 GiB memory in use. Of the allocated memory 6.79 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  21%|██        | 15/72 [00:07<00:28,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 59.50 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  22%|██▏       | 16/72 [00:08<00:27,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 47.50 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  24%|██▎       | 17/72 [00:08<00:28,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 47.06 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  25%|██▌       | 18/72 [00:09<00:27,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 2e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 49.12 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  26%|██▋       | 19/72 [00:09<00:26,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 49.38 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  28%|██▊       | 20/72 [00:10<00:26,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 50.00 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  29%|██▉       | 21/72 [00:10<00:25,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 50.69 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  31%|███       | 22/72 [00:11<00:25,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 51.12 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  32%|███▏      | 23/72 [00:11<00:24,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 47.06 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  33%|███▎      | 24/72 [00:12<00:24,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 46.19 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  35%|███▍      | 25/72 [00:12<00:23,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 46.50 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  36%|███▌      | 26/72 [00:13<00:22,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 65.62 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  38%|███▊      | 27/72 [00:13<00:22,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 72.62 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  39%|███▉      | 28/72 [00:14<00:21,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 73.06 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  40%|████      | 29/72 [00:14<00:21,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 70.88 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  42%|████▏     | 30/72 [00:15<00:20,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.01, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 70.88 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  43%|████▎     | 31/72 [00:15<00:20,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 70.94 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  44%|████▍     | 32/72 [00:16<00:19,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'albert-base-v2', 'lr': 5e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} : CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 71.88 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 6.99 GiB memory in use. Of the allocated memory 6.81 GiB is allocated by PyTorch, and 13.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "keys, values = zip(*grid.items())\n",
    "results = []\n",
    "for combo in tqdm(list(itertools.product(*values)), desc=\"Grid\"):\n",
    "    cfg = dict(zip(keys, combo))\n",
    "    try:\n",
    "        metrics = train_and_eval(cfg)\n",
    "        results.append({**cfg, **metrics})\n",
    "        print(\"CFG:\", cfg, \"=>\", metrics)\n",
    "    except Exception as e:\n",
    "        print(\"Error for cfg\", cfg, \":\", e)\n",
    "        results.append({**cfg, \"error\": str(e)})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"grid_results.csv\", index=False)\n",
    "print(\"Saved grid_results.csv\")\n",
    "best_idx = res_df[\"micro_f1\"].idxmax() if \"micro_f1\" in res_df.columns else None\n",
    "if best_idx is not None:\n",
    "    print(\"Best config:\\n\", res_df.loc[best_idx].to_dict())\n",
    "else:\n",
    "    print(\"No successful runs found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
