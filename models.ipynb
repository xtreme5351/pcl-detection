{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d37472",
   "metadata": {},
   "source": [
    "## Novel model approach\n",
    "\n",
    "This notebook serves as the source code for all the model testing and training (along with hyperparam grid search) before the development/submission of the final best model. This model approach tries a variation on the transformer architecture, with different heads, as detailed in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e213e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043011bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcl_tf.dataset_manager import DatasetManager as DM\n",
    "from pcl_tf.collation import collate_fn\n",
    "from pcl_tf.tf import warmup_model, get_tokenizer, _ENCODER_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9fd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 7\n",
    "LOAD_BATCH_SIZE = 64\n",
    "LOCAL_CACHE_DIR = './models_cache'\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = False\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd961ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up encoder (downloads model if needed)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00deea78253e4987b985c62de1193222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cache warmup completed.\n"
     ]
    }
   ],
   "source": [
    "print('Warming up tokenizer...')\n",
    "tokenizer = get_tokenizer(MODEL_NAME)\n",
    "\n",
    "print('Warming up encoder (downloads model if needed)...')\n",
    "\n",
    "_ = warmup_model(MODEL_NAME, device=None, cache_dir=LOCAL_CACHE_DIR)\n",
    "print('Model cache warmup completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f976d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_path = \"data/train_semeval_parids-labels.csv\"\n",
    "dev_labels_path = \"data/dev_semeval_parids-labels.csv\"\n",
    "texts_path = \"data/dontpatronizeme_pcl_cleaned.csv\"\n",
    "test_path = \"data/task4_test.tsv\"\n",
    "cats_path = \"data/dontpatronizeme_categories.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07510058",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv(texts_path, low_memory=False)\n",
    "texts_df[\"par_id\"] = texts_df[\"par_id\"].astype(int)\n",
    "texts_df = texts_df.set_index(\"par_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89398ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8375\n",
      "Binary distribution: [7581  794]\n",
      "Multilabel distribution: [574. 160. 162. 192. 145. 363.  29.]\n"
     ]
    }
   ],
   "source": [
    "training_ds = DM(train_labels_path, texts_df=texts_df)\n",
    "training_ds.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aee088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 label par_id(s) missing from texts_df. Example missing ids: [8640]\n",
      "Total samples: 2093\n",
      "Binary distribution: [1894  199]\n",
      "Multilabel distribution: [142.  36.  62.  38.  52. 106.  11.]\n"
     ]
    }
   ],
   "source": [
    "dev_ds = DM(dev_labels_path, texts_df=texts_df)\n",
    "dev_ds.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff0054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training_ds, batch_size=LOAD_BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=lambda b: collate_fn(tokenizer, b), pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)\n",
    "\n",
    "dev_loader = DataLoader(dev_ds, batch_size=LOAD_BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=lambda b: collate_fn(tokenizer, b), pin_memory=PIN_MEMORY, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66fc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dev(model, dataloader, device):\n",
    "    model.eval()\n",
    "    bin_probs=[]\n",
    "    bin_labels=[]\n",
    "    multi_probs=[]\n",
    "    multi_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in dataloader:\n",
    "            input_ids = b[\"input_ids\"].to(device)\n",
    "            attention_mask = b[\"attention_mask\"].to(device)\n",
    "            labels = b[\"labels\"].to(device)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bin_probs.append(torch.sigmoid(out[\"logit_bin\"]).cpu().numpy())\n",
    "            multi_probs.append(torch.sigmoid(out[\"logit_multi\"]).cpu().numpy())\n",
    "            bin_labels.append(labels[:,0].cpu().numpy())\n",
    "            multi_labels.append(labels[:,1:].cpu().numpy())\n",
    "            \n",
    "    bin_probs = np.concatenate(bin_probs); bin_labels = np.concatenate(bin_labels)\n",
    "    multi_probs = np.concatenate(multi_probs); multi_labels = np.concatenate(multi_labels)\n",
    "    micro_f1 = f1_score(multi_labels.flatten(), (multi_probs>=0.5).astype(int).flatten(), zero_division=0)\n",
    "    return {\"micro_f1\": micro_f1, \"bin_ap\": average_precision_score(bin_labels, bin_probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9616c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcl_tf.tf import PCLModel\n",
    "\n",
    "def train_and_eval(config):\n",
    "    model = PCLModel(config[\"model_name\"], n_labels=NUM_LABELS, dropout=config[\"dropout\"], device=DEVICE).to(DEVICE)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
    "    print(\"Model and optimizer created\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "            optim.zero_grad()\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running += loss.item()\n",
    "        # optionally print per-epoch\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running/len(train_loader)}\")\n",
    "    metrics = evaluate_dev(model, dev_loader, DEVICE)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "139d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"model_name\": [\"roberta-base\"],\n",
    "    \"lr\": [2e-5, 5e-5],\n",
    "    \"wd\": [1e-3, 1e-2, 5e-3],\n",
    "    \"max_len\": [128, 256],\n",
    "    \"dropout\": [0, 0.1],\n",
    "    \"epochs\": [6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1898784",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edaeed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.45656906318573554\n",
      "Epoch 2, Loss: 0.28324812241182984\n",
      "Epoch 3, Loss: 0.20947086287818792\n",
      "Epoch 4, Loss: 0.14777101606187473\n",
      "Epoch 5, Loss: 0.09722598522674036\n",
      "Epoch 6, Loss: 0.0835050648177853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   4%|▍         | 1/24 [07:16<2:47:23, 436.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.273224043715847, 'bin_ap': 0.5950159538533516}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4490828883784418\n",
      "Epoch 2, Loss: 0.29092072905929944\n",
      "Epoch 3, Loss: 0.22693791476483563\n",
      "Epoch 4, Loss: 0.15818579318641707\n",
      "Epoch 5, Loss: 0.1065674459291551\n",
      "Epoch 6, Loss: 0.07784430829111401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:   8%|▊         | 2/24 [14:33<2:40:12, 436.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3474178403755869, 'bin_ap': 0.6025107426492349}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.42959114203926263\n",
      "Epoch 2, Loss: 0.28572234716124206\n",
      "Epoch 3, Loss: 0.20815658296337564\n",
      "Epoch 4, Loss: 0.1454774436206763\n",
      "Epoch 5, Loss: 0.10083209019410246\n",
      "Epoch 6, Loss: 0.0794652417330569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  12%|█▎        | 3/24 [21:50<2:32:49, 436.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.3678832116788321, 'bin_ap': 0.6166042886003502}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.432477657349055\n",
      "Epoch 2, Loss: 0.2846253471349487\n",
      "Epoch 3, Loss: 0.2073063663000824\n",
      "Epoch 4, Loss: 0.13581463373230615\n",
      "Epoch 5, Loss: 0.09174259743503942\n",
      "Epoch 6, Loss: 0.07854629161533054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  17%|█▋        | 4/24 [29:07<2:25:35, 436.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3930942895086321, 'bin_ap': 0.6005527050861326}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4300794826664087\n",
      "Epoch 2, Loss: 0.29072908755704646\n",
      "Epoch 3, Loss: 0.22438651363130743\n",
      "Epoch 4, Loss: 0.1579709914121919\n",
      "Epoch 5, Loss: 0.1183913067153847\n",
      "Epoch 6, Loss: 0.0841311682584404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  21%|██        | 5/24 [36:23<2:18:19, 436.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.3645320197044335, 'bin_ap': 0.5850008568565164}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.44270398024156804\n",
      "Epoch 2, Loss: 0.2824859658952888\n",
      "Epoch 3, Loss: 0.20380870815906815\n",
      "Epoch 4, Loss: 0.13900417754895814\n",
      "Epoch 5, Loss: 0.0955747553069173\n",
      "Epoch 6, Loss: 0.07450716887095957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  25%|██▌       | 6/24 [43:40<2:11:01, 436.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.31186440677966104, 'bin_ap': 0.6033739355335254}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.44939536527367946\n",
      "Epoch 2, Loss: 0.2958022359445805\n",
      "Epoch 3, Loss: 0.22372344559954324\n",
      "Epoch 4, Loss: 0.1595347821712494\n",
      "Epoch 5, Loss: 0.10885829555283066\n",
      "Epoch 6, Loss: 0.08455285574984915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  29%|██▉       | 7/24 [50:57<2:03:43, 436.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.36123348017621143, 'bin_ap': 0.5887737848535188}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.44479305757821064\n",
      "Epoch 2, Loss: 0.2892388499533857\n",
      "Epoch 3, Loss: 0.20778586032963892\n",
      "Epoch 4, Loss: 0.14224391972347525\n",
      "Epoch 5, Loss: 0.09643616069477932\n",
      "Epoch 6, Loss: 0.07566344597241806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  33%|███▎      | 8/24 [58:13<1:56:27, 436.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3387096774193548, 'bin_ap': 0.6214037642348211}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4254075916668841\n",
      "Epoch 2, Loss: 0.28034869928396383\n",
      "Epoch 3, Loss: 0.2049034678355883\n",
      "Epoch 4, Loss: 0.13171733196341354\n",
      "Epoch 5, Loss: 0.09289591395445453\n",
      "Epoch 6, Loss: 0.08069981159713432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  38%|███▊      | 9/24 [1:05:30<1:49:08, 436.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.37567567567567567, 'bin_ap': 0.6095496883839383}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.45806975776457604\n",
      "Epoch 2, Loss: 0.29003186523914337\n",
      "Epoch 3, Loss: 0.21735542648621187\n",
      "Epoch 4, Loss: 0.15448838128274633\n",
      "Epoch 5, Loss: 0.10809075929053867\n",
      "Epoch 6, Loss: 0.0725292632398942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  42%|████▏     | 10/24 [1:12:47<1:41:53, 436.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3785234899328859, 'bin_ap': 0.588893416033394}\n",
      "Model and optimizer created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  46%|████▌     | 11/24 [1:12:49<1:05:49, 303.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for cfg {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0, 'epochs': 6} : CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 74.00 MiB is free. Process 6267 has 37.41 MiB memory in use. Process 6351 has 39.41 MiB memory in use. Process 44088 has 21.16 MiB memory in use. Including non-PyTorch memory, this process has 7.00 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 684.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4572733736220207\n",
      "Epoch 2, Loss: 0.3011042355126097\n",
      "Epoch 3, Loss: 0.24049579664951062\n",
      "Epoch 4, Loss: 0.17803256648756166\n",
      "Epoch 5, Loss: 0.11983354762196541\n",
      "Epoch 6, Loss: 0.10198159218687593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  50%|█████     | 12/24 [1:20:06<1:08:51, 344.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3395061728395062, 'bin_ap': 0.5823587998462407}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.402828793489296\n",
      "Epoch 2, Loss: 0.2732160648771825\n",
      "Epoch 3, Loss: 0.18088675446751465\n",
      "Epoch 4, Loss: 0.11908478433451125\n",
      "Epoch 5, Loss: 0.09738358202610763\n",
      "Epoch 6, Loss: 0.07412350273740884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  54%|█████▍    | 13/24 [1:27:23<1:08:16, 372.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.3559096945551129, 'bin_ap': 0.5864831265279478}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4329040566928514\n",
      "Epoch 2, Loss: 0.30127211222211825\n",
      "Epoch 3, Loss: 0.2200675446341056\n",
      "Epoch 4, Loss: 0.15053664568726344\n",
      "Epoch 5, Loss: 0.11192177881607572\n",
      "Epoch 6, Loss: 0.08282499665124271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  58%|█████▊    | 14/24 [1:34:40<1:05:18, 391.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.001, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.30560271646859083, 'bin_ap': 0.5626739053148456}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.41875473335954067\n",
      "Epoch 2, Loss: 0.2827950474870114\n",
      "Epoch 3, Loss: 0.21356117148094506\n",
      "Epoch 4, Loss: 0.1368024042942597\n",
      "Epoch 5, Loss: 0.10277063852417788\n",
      "Epoch 6, Loss: 0.06543721501779465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  62%|██████▎   | 15/24 [1:41:57<1:00:48, 405.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.3122923588039867, 'bin_ap': 0.5201969615081425}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4296468081137606\n",
      "Epoch 2, Loss: 0.2970080159547675\n",
      "Epoch 3, Loss: 0.2352227999167588\n",
      "Epoch 4, Loss: 0.1642850016648988\n",
      "Epoch 5, Loss: 0.10474172828875425\n",
      "Epoch 6, Loss: 0.0890571853316581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  67%|██████▋   | 16/24 [1:49:13<55:17, 414.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.24285714285714285, 'bin_ap': 0.5499711742720691}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4050332345352828\n",
      "Epoch 2, Loss: 0.2858610772563301\n",
      "Epoch 3, Loss: 0.20574631888902825\n",
      "Epoch 4, Loss: 0.14121462367759405\n",
      "Epoch 5, Loss: 0.11471200666127314\n",
      "Epoch 6, Loss: 0.08334310376974008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  71%|███████   | 17/24 [1:56:30<49:10, 421.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.3151515151515151, 'bin_ap': 0.5561576998732162}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4369251763320151\n",
      "Epoch 2, Loss: 0.30090474780961757\n",
      "Epoch 3, Loss: 0.2325960752618222\n",
      "Epoch 4, Loss: 0.15377018279360452\n",
      "Epoch 5, Loss: 0.10689818864787808\n",
      "Epoch 6, Loss: 0.0707852357628805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  75%|███████▌  | 18/24 [2:03:46<42:35, 425.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.01, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.33921302578018997, 'bin_ap': 0.4951717095947703}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4593095365371413\n",
      "Epoch 2, Loss: 0.34737776822716226\n",
      "Epoch 3, Loss: 0.2825833195607171\n",
      "Epoch 4, Loss: 0.2160498268674803\n",
      "Epoch 5, Loss: 0.16516919235009273\n",
      "Epoch 6, Loss: 0.11161234081928967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  79%|███████▉  | 19/24 [2:11:03<35:45, 429.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.21830985915492956, 'bin_ap': 0.4929176806834262}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4326106676163564\n",
      "Epoch 2, Loss: 0.3193895636510303\n",
      "Epoch 3, Loss: 0.231781434239322\n",
      "Epoch 4, Loss: 0.1541283964712656\n",
      "Epoch 5, Loss: 0.10370032762298147\n",
      "Epoch 6, Loss: 0.07708807863323515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  83%|████████▎ | 20/24 [2:18:20<28:46, 431.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.01, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.2669039145907473, 'bin_ap': 0.5224543986571739}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.43127605546521774\n",
      "Epoch 2, Loss: 0.32729125136637505\n",
      "Epoch 3, Loss: 0.2482476328279226\n",
      "Epoch 4, Loss: 0.18014279349159648\n",
      "Epoch 5, Loss: 0.1322674640091776\n",
      "Epoch 6, Loss: 0.0986728637784947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  88%|████████▊ | 21/24 [2:25:38<21:39, 433.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.34065934065934067, 'bin_ap': 0.5733371131403427}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4306029257656054\n",
      "Epoch 2, Loss: 0.3114978073321226\n",
      "Epoch 3, Loss: 0.23258725590719523\n",
      "Epoch 4, Loss: 0.15439212814200926\n",
      "Epoch 5, Loss: 0.09600465111292274\n",
      "Epoch 6, Loss: 0.08109679958918167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  92%|█████████▏| 22/24 [2:32:54<14:28, 434.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.005, 'max_len': 128, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.3286384976525822, 'bin_ap': 0.5540698397455713}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4094110709679036\n",
      "Epoch 2, Loss: 0.2792855397094297\n",
      "Epoch 3, Loss: 0.20516147908136134\n",
      "Epoch 4, Loss: 0.13374803553669506\n",
      "Epoch 5, Loss: 0.09884872769524577\n",
      "Epoch 6, Loss: 0.07762129542254309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid:  96%|█████████▌| 23/24 [2:40:11<07:14, 434.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0, 'epochs': 6} => {'micro_f1': 0.31347962382445144, 'bin_ap': 0.5089862991043769}\n",
      "Model and optimizer created\n",
      "Starting training...\n",
      "Epoch 1, Loss: 0.4027164536350556\n",
      "Epoch 2, Loss: 0.29668836835913986\n",
      "Epoch 3, Loss: 0.21055090225493636\n",
      "Epoch 4, Loss: 0.15677081836202672\n",
      "Epoch 5, Loss: 0.10435222879848408\n",
      "Epoch 6, Loss: 0.08140688053979218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid: 100%|██████████| 24/24 [2:47:28<00:00, 418.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'model_name': 'roberta-base', 'lr': 5e-05, 'wd': 0.005, 'max_len': 256, 'dropout': 0.1, 'epochs': 6} => {'micro_f1': 0.30973451327433627, 'bin_ap': 0.5267868684231509}\n",
      "Saved grid_results.csv\n",
      "Best config:\n",
      " {'model_name': 'roberta-base', 'lr': 2e-05, 'wd': 0.001, 'max_len': 256, 'dropout': 0.1, 'epochs': 6, 'micro_f1': 0.3930942895086321, 'bin_ap': 0.6005527050861326, 'error': nan}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keys, values = zip(*grid.items())\n",
    "results = []\n",
    "for combo in tqdm(list(itertools.product(*values)), desc=\"Grid\"):\n",
    "    cfg = dict(zip(keys, combo))\n",
    "    try:\n",
    "        metrics = train_and_eval(cfg)\n",
    "        results.append({**cfg, **metrics})\n",
    "        print(\"CFG:\", cfg, \"=>\", metrics)\n",
    "    except Exception as e:\n",
    "        print(\"Error for cfg\", cfg, \":\", e)\n",
    "        results.append({**cfg, \"error\": str(e)})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"grid_results.csv\", index=False)\n",
    "print(\"Saved grid_results.csv\")\n",
    "best_idx = res_df[\"micro_f1\"].idxmax() if \"micro_f1\" in res_df.columns else None\n",
    "if best_idx is not None:\n",
    "    print(\"Best config:\\n\", res_df.loc[best_idx].to_dict())\n",
    "else:\n",
    "    print(\"No successful runs found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
